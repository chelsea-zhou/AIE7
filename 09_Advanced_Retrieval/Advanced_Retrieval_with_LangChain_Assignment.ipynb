{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"s09-d3cc956a\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"  \n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, one of the most common issues with student loans appears to be mismanagement and errors by loan servicers. These issues include:\\n\\n- Errors in loan balances and incorrect reporting on credit reports.\\n- Difficulty applying payments properly, often resulting in unwanted interest accrual and inability to pay down principal.\\n- Lack of clear communication and transparency regarding loan terms, transfers, and balances.\\n- Problems with loan handling, such as unauthorized transfers, mishandling of forbearances, and inaccurate account statuses.\\n- Disputes over incorrect information, late payments, and issues with loan forgiveness or discharge.\\n\\nTherefore, a most common issue is **mismanagement and inaccuracies by loan servicers, including errors in balances, incorrect reporting, and inadequate communication**.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, at least two complaints indicate delayed responses:\\n\\n1. Complaint from 03/28/25 (submitted to Mohela), where the response was marked as 'No' for timely response, showing the consumer's expectations for a timely resolution were not met.\\n2. Complaint from 04/24/25 (submitted to Maximus Federal Services) also responded 'Yes' for timely response, but multiple other complaints, such as the one from 04/14/25 to Nelnet, indicate the issue was not addressed promptly, with delays longer than originally promised.\\n\\nOverall, the complaints highlight instances where consumer concerns about delays or lack of response were evident, implying that some complaints were not handled in a timely manner.\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons based on the provided complaints:\\n\\n1. **Lack of clear communication and notification:** Many complainants were not adequately informed about when their loan repayment was to resume, loan transfer details, or changes in their loan status. For example, some were unaware their loans had been transferred to different servicers or were not notified when repayment was supposed to start.\\n\\n2. **Difficulty with repayment options:** Several individuals faced limited options like forbearance or deferment, which led to accumulating interest and increasing the total debt. Some felt these options extended the repayment period and made paying off the loans more difficult.\\n\\n3. **Financial hardships and inability to afford payments:** Many borrowers indicated that increasing payments to reduce principal was unaffordable, especially given stagnant wages, inflation, or other financial hardships. This made repayment seem unmanageable.\\n\\n4. **Problems with loan servicing practices:** Complaints include issues like being unable to apply extra payments to principal, servicers applying payments primarily to interest, or being placed on long-term forbearance without proper handling, which prolonged debt.\\n\\n5. **Borrower misunderstandings or lack of information:** Some borrowers were unaware of their repayment obligations, or how interest and balances accrue, leading to unintentional delinquencies or credit issues.\\n\\n6. **Errors and mismanagement:** Complaints also involve incorrect reporting of late payments, inability to access account information, or transfer of loans without proper notification, contributing to difficulties in managing loan repayment.\\n\\nIn summary, failure to repay was often due to a combination of inadequate communication, financial constraints, complex or unfavorable servicing practices, and administrative errors.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to the handling and servicing of the loans, such as dealing with lenders or servicers, receiving incorrect or bad information about the loans, and issues with repayment processes. Specific recurring issues include disputes over fees charged, difficulty applying payments correctly, and confusion about loan balances and terms.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, it appears that several complaints were responded to in a timely manner, with the responses marked as \"Yes\" for \"Timely response?\". Specifically, complaints received on 04/01/25, 04/24/25, and 04/26/25 indicate timely responses from the companies involved. \\n\\nHowever, there are complaints (e.g., the one received on 05/08/25) where the complaint response was marked as \"Closed with explanation,\" but the narrative suggests ongoing issues and dissatisfaction, and there is no indication they were handled outside of the overall timely response window. \\n\\nSince the data does not record any complaints that explicitly remained unresolved or claimed to be handled late, there is no evidence in this dataset that any complaints were not handled in a timely manner. \\n\\nTherefore, the answer is:  \\n**No, there is no indication that any complaints did not get handled in a timely manner.**'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People often fail to pay back their loans due to a variety of issues, including mismanagement or errors by loan servicers, lack of communication, or complications with their payment plans. For example, some individuals experienced their autopayments being discontinued without proper notification, leading to missed payments and negative credit impacts. Others faced confusion and difficulties when their loans were transferred between different companies, often without clear communication, which resulted in lack of awareness about their payment status or due dates. Additionally, some borrowers had trouble with loan repayment plans or forbearances due to administrative problems, such as their requests not being handled properly or being ignored altogether. These issues can lead to unintentional default or missed payments, making it seem like borrowers failed to pay back their loans, when in fact the failures stemmed from administrative errors, poor communication, or inadequate support from the loan servicers.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ✅ answer 1 \n",
        "\n",
        "Example Query: \"Wells Fargo student loan payment processing error\"\n",
        "\n",
        "\n",
        "#### 1. **Exact Entity Matching**\n",
        "- **BM25** finds documents containing the exact term **\"Wells Fargo\"**\n",
        "- **Embeddings** might return results about other banks (Chase, Bank of America) due to semantic similarity\n",
        "- **Critical**: Users need information about their specific financial institution\n",
        "\n",
        "#### 2. **Precise Product Type Matching**\n",
        "- **BM25** ensures exact match for **\"student loan\"** (not auto loans, mortgages, credit cards)\n",
        "- **Embeddings** might conflate different loan types due to semantic similarity\n",
        "- **Important**: Different loan types have different regulations and procedures\n",
        "\n",
        "#### 3. **Specific Issue Identification**\n",
        "- **BM25** targets exact phrase **\"payment processing error\"**\n",
        "- **Embeddings** might return general payment issues, billing problems, or system errors\n",
        "- **Crucial**: Specific error types require specific solutions\n",
        "\n",
        "#### 4. **Domain-Specific Terminology**\n",
        "- Financial services use **precise terminology** with legal implications\n",
        "- **BM25** preserves exact language from regulations and official documentation\n",
        "- **Embeddings** might generalize technical terms, missing important distinctions\n",
        "\n",
        "\n",
        "**Conclusion**: For queries requiring **exact terminology, specific entities, or precise product matching**, BM25's keyword-based approach outperforms embeddings' semantic generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided context, appears to be problems related to \"Dealing with your lender or servicer,\" including issues such as receiving bad information about loans, errors in loan balances, misapplied payments, wrongful denials of payment plans, and complications arising from how payments are handled or processed. Many complaints highlight difficulties with understanding or managing interest accumulation, unclear or conflicting information about balances, and challenges with repayment options like forbearance or deferment that can lead to increased interest over time.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, at least one complaint indicates it was not handled in a timely manner. Specifically, the complaint about the student loan account review, which has been open for nearly 18 months without resolution, suggests a significant delay. The complaint notes that the individual has been awaiting response and resolution for over a year, and there has been no resolution despite the long duration.\\n\\nHowever, for other complaints, the responses from the companies are noted as \"Closed with explanation\" and marked as \"Yes\" for timely response, indicating they were addressed within an acceptable timeframe.\\n\\nTherefore, yes, there was at least one complaint that was not handled in a timely manner.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a lack of clear information and communication about their loans, as well as difficulties managing interest and payment options. Many borrowers were not informed about the necessity of repayment or the details of how interest accrues over time, especially when loans are transferred or serviced by different companies without proper notification. Additionally, some borrowers faced limited options—such as only being offered forbearance or deferment—that resulted in ongoing interest accumulation, making it harder to repay the principal. Other reasons include financial hardships, stagnating wages, and misguidance about repayment terms and forgiveness programs, all of which contributed to their inability to fulfill repayment obligations.\\n'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The most common issue with loans, based on the provided complaints and context, appears to be problems related to the handling and servicing of student loans. This includes:\\n\\n- Dealing with lenders or servicers who mishandle payments, misapply payments, or apply them incorrectly (e.g., applying to interest instead of principal).\\n- Errors in loan balances and interest calculations.\\n- Lack of transparent communication or failure to provide necessary documentation like original promissory notes.\\n- Unjustified increases in interest rates or balances, often due to forbearances or transfers between servicers.\\n- Problems with loan repayment plans, such as being steered into unsuitable options or facing difficulty in applying additional payments to principal.\\n- Erroneous or unauthorized loans appearing on credit reports.\\n- Servicers' failure to verify or maintain proper legal documentation, such as signed master promissory notes.\\n\\nOverall, the most prevalent issue is the mishandling and mismanagement of loan servicing, leading to errors, misinformation, and adverse impacts on borrowers' credit and finances.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, one complaint received a response marked as \"No\" for timely response, indicating it was late. Additionally, there are multiple instances where complainants reported waiting over long periods (hours) without resolution, or their issues remained unresolved for over a year despite multiple follow-ups. Therefore, it can be concluded that certain complaints were not addressed promptly.'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors highlighted in these complaints:\\n\\n1. **Accumulation of Interest and Unmanageable Balances**: Many borrowers reported that interest continued to accrue during forbearance or deferment periods, sometimes capitalizing (adding to principal), which increased the total amount owed and made repayment more difficult.\\n\\n2. **Lack of Clear and Accurate Information from Servicers**: Several complaints cite servicers steering borrowers into forbearance or consolidations without informing them of better options like income-driven repayment plans or rehabilitation, leading to increased balances and loss of forgiveness eligibility.\\n\\n3. **Financial Hardship and Economic Conditions**: Borrowers faced hardships such as unemployment, low income, or unexpected expenses, making it physically or financially impossible to increase monthly payments without sacrificing essentials.\\n\\n4. **Mismanagement and Lack of Transparency**: Reports include issues like incorrect account statuses, misapplied payments, errors in balances, or transfer of loans without proper notification, all of which hindered borrowers’ ability to manage and repay their loans effectively.\\n\\n5. **Procedural and Servicing Practices**: Tactics such as “forbearance steering,” coercive consolidation, and inadequate guidance about repayment options contributed to the inability to qualify for loan forgiveness or to reduce debt burdens.\\n\\n6. **Inadequate Support and Communication**: Many borrowers experienced unhelpful customer service, received little to no notice about loan status changes, or were misled about options, leading to default or increased difficulty in repayment.\\n\\nIn summary, the failure to pay back loans was driven by the combination of rising balances due to interest accrual, lack of proper information and support, economic hardship, and systemic issues within student loan servicing practices.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ✅ answer 2\n",
        "\n",
        "Multi-query retrieval improves recall by solving the vocabulary mismatch problem between user queries and document content. When a user asks \"What are loan default rates?\", the LLM generates multiple reformulations like \"How often do borrowers fail to repay loans?\" and \"Loan delinquency statistics,\" each creating different embedding vectors that cover broader areas of the semantic space. \n",
        "\n",
        "Since documents may use varying terminology (\"default,\" \"delinquency,\" \"non-payment\"), each reformulation has the potential to match different relevant documents that the original query might miss. This approach significantly expands the retrieval pool - instead of finding 10 documents from a single query, multiple reformulations can discover 25+ unique relevant documents after deduplication. Our evaluation confirms this effectiveness, with multi-query achieving 97.5%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the complaints provided, appears to be related to errors and misconduct in federal student loan servicing. Specific recurring problems include incorrect information on credit reports, misapplication of payments, wrongful denials of payment plans, discrepancies in loan balances and interest rates, and issues with collection and verification of debts.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, it appears that several complaints were not handled in a timely manner. Specifically, the complaints related to the student loan issues with MOHELA (Complaint IDs 12709087 and 12935889) indicate that the responses were \"No\" in the \"Timely response?\" field, meaning they were not handled promptly. Additionally, the complaint about the dispute settlement with Nelnet (Complaint ID 13205525) was responded to within the expected timeframe (\"Yes\" in \"Timely response?\"). \\n\\nTherefore, yes, some complaints—particularly those regarding MOHELA—did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for various reasons, including:\\n\\n1. Lack of proper communication or notification from loan servicers about payment obligations, as indicated by complaints about not being notified when payments were due or about changes in loan ownership.\\n2. Financial hardship or severe economic difficulties that made it impossible to make timely payments, such as unemployment or inability to find employment in their field.\\n3. Misrepresentation or lack of transparency from educational institutions and loan providers regarding the long-term financial consequences, job prospects after graduation, and the sustainability of the school’s operations.\\n4. Relying on deferment and forbearance options that increased interest and debt over time.\\n5. Disputes over the legitimacy or ownership of the debt, including issues related to the legal verification of loans and deceptive practices by collection agencies.\\n6. Personal health issues or other personal circumstances that hindered ability to make payments.\\n\\nIn summary, failure to repay loans often resulted from a combination of financial hardship, lack of clear communication, and sometimes the mismanagement or misrepresentation by educational and loan servicing entities.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided data, appears to be dealing with the loan servicer or lender, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and problems with how payments are being handled. Several complaints highlight issues such as receiving bad information about loans, inability to properly apply payments to principal, inaccurate reporting of delinquency, and mishandling of loan transfers or consolidations. \\n\\nIn summary, a predominant and recurring problem is the mismanagement and poor communication from loan servicers, which leads to misapplied payments, incorrect account information, and difficulties in resolving repayment issues.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, yes, there are several instances indicating complaints not handled in a timely manner. For example:\\n\\n- One complaint (#12935889) about Mohela was marked as \"Timely response?\": No.\\n- Another (#12744910) regarding inaccuracies in reporting and an ongoing dispute was \"Timely response?\": Yes, but the complaint was about inaccurate reporting and delays in correction, suggesting the issue persisted over time.\\n- Multiple complaints (#12739706, #13062402, #13126709, #13127090, and others) mention delays, extended wait times, or responses that were not addressed promptly, with some even explicitly stating they did not receive responses within expected timeframes.\\n- There are cases where the response was \"Closed with explanation\" but the delays or unresolved issues strongly imply they were not handled promptly or adequately.\\n\\nOverall, the evidence suggests that at least some complaints were not handled in a timely manner, as indicated directly by the response statuses and detailed descriptions of delays.'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans for several reasons, often related to mismanagement, misinformation, and systemic issues. Based on the provided complaints, common reasons include:\\n\\n1. **Lack of Notification and Communication:** Many borrowers were not properly notified about loan transfers, due dates, or repayment start dates, leading to unintentional delinquency and missed payments.\\n\\n2. **Misleading or Incomplete Information:** Borrowers reported receiving incorrect or misleading information about their loan balances, repayment obligations, or eligibility for programs like income-driven repayment or forgiveness, which caused confusion and unintended default.\\n\\n3. **System Errors and Technical Difficulties:** Issues such as online portal lockouts, incorrect account statuses, and errors in reporting contributed to borrowers not making payments or being marked delinquent improperly.\\n\\n4. **Inadequate Support and Assistance:** Borrowers often found customer service unhelpful, unresponsive, or dismissive, preventing them from arranging manageable payment plans or resolving discrepancies.\\n\\n5. **Financial Hardship and Economic Factors:** Some borrowers experienced severe financial difficulties, unemployment, or health issues that made repayment impossible despite intentions to pay.\\n\\n6. **Systemic and Administrative Failures:** Transitions between servicers, erroneous reporting to credit bureaus, and failure to follow regulations led to credit damage and further complications in repayment.\\n\\nIn summary, failures to pay back loans often stemmed from a combination of administrative mismanagement, inadequate communication, lack of clarity about repayment options, and financial hardships.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, the most common issues with loans appear to be related to difficulties in communication and account management, such as:\\n\\n- Struggling to repay loans due to errors or issues with payment plans.\\n- Problems with loan reporting, including incorrect or improper reporting of account status or default.\\n- Difficulties in obtaining clear information about loan balances, loan servicer changes, or payment amounts.\\n- Issues with loan servicing companies failing to respond appropriately or failing to verify or process applications.\\n- Unauthorized or illegal reporting and collection practices, including violations of privacy laws.\\n\\nWhile these are specific to student loans in the context provided, a recurring theme is that many complaints involve mismanagement, lack of transparency, or errors in the handling of loans and related information. \\n\\nTherefore, a common underlying issue with loans, especially highlighted here, is **mismanagement or errors in servicing, reporting, or communication that cause confusion, financial hardship, or violations of legal protections.**'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints, it appears that many complaints were responded to in a timely manner, with responses marked as \\'Yes\\' under the \\'Timely response?\\' field. Notably, several complaints state \"Closed with explanation,\" indicating that they were addressed within the required time frame. \\n\\nHowever, there is at least one complaint regarding a lack of response or handling—specifically, the complaint about Nelnet (row 17). The consumer\\'s narrative details multiple issues with lack of responses and conduct that suggests their complaint was not handled promptly or satisfactorily.\\n\\nIn summary:\\n\\n- Multiple complaints confirm responses were handled in a timely manner.\\n- One complaint (about Nelnet\\'s failure to respond to Certified Mail and ongoing misconduct) indicates that the complaint was not properly handled or responded to, suggesting that some complaints did not get handled in a timely manner.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner, specifically the complaint involving Nelnet\\'s lack of response to legal and misconduct issues.'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including issues such as difficulties dealing with their loan servicers, miscommunications or inadequate information about their loan status, problems with payment processing, and disputes over the legitimacy or accuracy of their loan details. Some specific reasons noted in the complaints include receiving bad information about loan statuses, delays or errors in re-amortizing payments after forbearance ended, and inaccurate reports of default or delinquency. Additionally, instances of alleged mismanagement, lack of transparency, or improper handling of personal data have also contributed to borrowers' difficulties in repayment.\""
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ✅ answer 3 \n",
        "\n",
        "### How Semantic Chunking Would Behave with FAQs:\n",
        "\n",
        "#### **Problems with Short, Repetitive Sentences**:\n",
        "\n",
        "1. **Low Semantic Differentiation**: Short sentences contain limited semantic information, making embeddings very similar across different questions\n",
        "2. **False Groupings**: Questions like \"How do I apply?\" and \"How do I pay?\" might cluster together due to similar structure rather than semantic meaning\n",
        "3. **Threshold Confusion**: High similarity scores between repetitive patterns make it difficult to determine meaningful chunk boundaries\n",
        "4. **Over-fragmentation**: Algorithm might split related Q&As that use slightly different phrasing for the same concept\n",
        "\n",
        "\n",
        "### Algorithm Adjustments:\n",
        "\n",
        "#### **1. Change Thresholding Method**\n",
        "- **Current**: `percentile` (based on distance distribution)\n",
        "- **Better for FAQs**: `gradient` - detects sharp changes in similarity rather than overall distribution\n",
        "- **Rationale**: Finds natural topic boundaries even when overall similarities are high\n",
        "\n",
        "#### **2. Incorporate Question-Answer Pairing**\n",
        "```python\n",
        "# Instead of chunking by sentences, chunk by Q&A pairs\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"gradient\",\n",
        "    sentence_split_regex=r\"(?<=\\?)\\s+(?=[A-Z])\"  # Split on question boundaries\n",
        ")\n",
        "```\n",
        "\n",
        "#### **3. Pre-process for FAQ Structure**\n",
        "- **Parse Q&A pairs**: Treat each question-answer as atomic unit\n",
        "- **Use question embeddings only**: Chunk based on question similarity, not answer text\n",
        "- **Category-aware chunking**: Group by predefined categories (payments, applications, etc.)\n",
        "\n",
        "#### **4. Adjust Similarity Threshold**\n",
        "- **Lower threshold**: More sensitive to small semantic differences\n",
        "- **Use multiple metrics**: Combine cosine similarity with keyword overlap\n",
        "- **Domain-specific embeddings**: Use FAQ-trained embeddings that better differentiate short queries\n",
        "\n",
        "#### **5. Hybrid Approach**\n",
        "```python\n",
        "# Combine semantic similarity with structural patterns\n",
        "def faq_semantic_chunker(documents):\n",
        "    # First: Group by question structure patterns\n",
        "    # Second: Apply semantic chunking within each structural group\n",
        "    # Third: Merge semantically similar groups\n",
        "```\n",
        "\n",
        "### **Recommended Solution**:\n",
        "Switch to **gradient-based thresholding** with **Q&A pair preservation** and **lower similarity thresholds** to maintain FAQ structure while achieving meaningful semantic groupings based on topic rather than linguistic patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ✅ Answer \n",
        "| Retriever | Avg Latency (s) | Total Cost ($) | Tokens | Context Recall | Context Precision |\n",
        "|-----------|----------------|----------------|--------|----------------|-------------------|\n",
        "| **ensemble** | 10.94 | $0.02 | 177,088 | 1.0000 | 0.8709 |\n",
        "| **contextual_compression** | 4.39 | $0.01 | 28,620 | 0.7871 | 0.8917 |\n",
        "| **parent_document** | 5.13 | $0.01 | 43,063 | 0.8136 | 0.9333 |\n",
        "| **multi_query** | 9.67 | $0.02 | 129,223 | 0.9750 | 0.8374 |\n",
        "| **bm25** | 3.99 | $0.01 | 46,168 | 0.9071 | 0.8417 |\n",
        "| **naive** | 7.47 | $0.01 | 80,432 | 0.9300 | 0.8904 |\n",
        "\n",
        "### Speed vs Quality Trade-off\n",
        "- Fastest: BM25 (3.99s) with good recall (0.907) but moderate precision (0.842)\n",
        "- Slowest: Ensemble (10.94s) but perfect recall (1.0) - finds everything\n",
        "- Sweet Spot: Contextual Compression (4.39s) - 2nd fastest despite using LLM for reranking\n",
        "### Cost Efficiency Analysis\n",
        "- 6x Token Difference: Ensemble (177k) vs Contextual Compression (28k tokens)\n",
        "- Expensive Methods: Multi-Query (129k) & Ensemble (177k) due to multiple LLM calls\n",
        "- Most Efficient: Contextual Compression - lowest token usage while maintaining decent quality\n",
        "### Quality Performance\n",
        "Recall Champions (Completeness):\n",
        "- Ensemble: 1.0000 (perfect - finds everything)\n",
        "- Multi-Query: 0.9750 (97.5% - multiple query expansion works)\n",
        "- Naive: 0.9300 (strong baseline)\n",
        " \n",
        "Precision Champions (Accuracy):\n",
        "- Parent Document: 0.9333 (93% relevant results)\n",
        "- Contextual Compression: 0.8917 (89% - reranking filters noise)\n",
        "- Naive: 0.8904 (solid baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Generate Synthetic Test Dataset using Ragas\n",
        "print(\"Generating synthetic test dataset...\")\n",
        "\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "\n",
        "# Initialize the testset generator with our LLM and embedding model\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "\n",
        "\n",
        "# Use a subset of documents for test generation (to manage cost)\n",
        "test_documents = loan_complaint_data[:20]  # Using first 50 documents\n",
        "\n",
        "# Generate test dataset\n",
        "testset = generator.generate_with_langchain_docs(\n",
        "    documents=test_documents,\n",
        "    testset_size=10 # Generate 10 question-answer pairs\n",
        ")\n",
        "\n",
        "# Convert to pandas DataFrame for easier handling\n",
        "test_df = testset.to_pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "from langsmith.run_helpers import traceable\n",
        "from langsmith import Client\n",
        "\n",
        "\n",
        "def enrich_dataset(graph):\n",
        "    dataset = copy.deepcopy(testset)\n",
        "    for test_row in dataset:\n",
        "        response = graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "        test_row.eval_sample.response = response[\"response\"].content\n",
        "        test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "    return dataset \n",
        "\n",
        "def run_ragas_evaluation(dataset):\n",
        "    evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "    custom_run_config = RunConfig(timeout=360)\n",
        "    result = evaluate(\n",
        "        dataset=evaluation_dataset,\n",
        "        metrics=[LLMContextRecall(), ContextPrecision()],\n",
        "        llm=evaluator_llm,\n",
        "        run_config=custom_run_config\n",
        "    )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from langsmith import Client, tracing_context\n",
        "from datetime import datetime\n",
        "\n",
        "# Store results\n",
        "evaluation_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🚀 Starting Retriever Evaluation with Cost/Latency Tracking\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize LangSmith client\n",
        "client = Client()\n",
        "\n",
        "# Turn off global tracing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
        "\n",
        "# Dictionary of all your retrieval chains\n",
        "retrieval_chains = {\n",
        "    # \"naive\": naive_retrieval_chain,\n",
        "    # \"bm25\": bm25_retrieval_chain, \n",
        "    # \"multi_query\": multi_query_retrieval_chain,\n",
        "    # \"parent_document\": parent_document_retrieval_chain,\n",
        "    # \"contextual_compression\": contextual_compression_retrieval_chain,\n",
        "    \"ensemble\": ensemble_retrieval_chain\n",
        "    #\"semantic\": semantic_retrieval_chain\n",
        "}\n",
        "\n",
        "for retriever_name, chain in retrieval_chains.items():\n",
        "    print(f\"\\n📊 Evaluating: {retriever_name.upper()}\")\n",
        "    \n",
        "    # Create unique project name for this retriever\n",
        "    project_name = f\"s09-retriever-eval-{retriever_name}\"\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Trace only this specific evaluation\n",
        "    with tracing_context(\n",
        "        enabled=True,\n",
        "        project_name=project_name,\n",
        "        tags=[f\"retriever:{retriever_name}\", \"evaluation\"]\n",
        "    ):\n",
        "        dataset = enrich_dataset(chain)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    \n",
        "    print(f\"   ✅ Completed in {total_time:.2f} seconds\")\n",
        "    print(f\"   📁 Traces in project: {project_name}\")\n",
        "    \n",
        "    # Store basic metrics\n",
        "    evaluation_results[retriever_name] = {\n",
        "        \"dataset\": dataset,\n",
        "        \"total_time\": total_time,\n",
        "        \"project_name\": project_name,\n",
        "        \"num_samples\": len(dataset)\n",
        "    }\n",
        "\n",
        "print(f\"\\n✅ All evaluations completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract detailed metrics from LangSmith traces\n",
        "def get_langsmith_metrics(project_name, retriever_name):\n",
        "    \"\"\"Extract cost and latency metrics from LangSmith project\"\"\"\n",
        "    try:\n",
        "        # Get all runs from the project\n",
        "        runs = list(client.list_runs(project_name=project_name))\n",
        "        \n",
        "        if not runs:\n",
        "            return {\"error\": \"No runs found\"}\n",
        "        \n",
        "        parent_runnable_runs = [\n",
        "            run for run in runs \n",
        "            if run.name == 'RunnableSequence' \n",
        "            and run.parent_run_id is None  # Only parent runs\n",
        "        ]\n",
        "        \n",
        "        print(f\"Found {len(parent_runnable_runs)} parent RunnableSequence runs for {retriever_name}\")\n",
        "\n",
        "        # Calculate metrics\n",
        "        total_cost = 0\n",
        "        total_latency = 0\n",
        "        llm_calls = 0\n",
        "        \n",
        "        for run in parent_runnable_runs:\n",
        "            # Cost (if available)\n",
        "            if hasattr(run, 'total_cost') and run.total_cost:\n",
        "                total_cost += run.total_cost\n",
        "                \n",
        "            # Latency \n",
        "            if run.end_time and run.start_time:\n",
        "                latency = (run.end_time - run.start_time).total_seconds()\n",
        "                total_latency += latency\n",
        "                \n",
        "            # Count LLM calls\n",
        "            if run.run_type == \"llm\":\n",
        "                llm_calls += 1\n",
        "        \n",
        "        num_chains = len(parent_runnable_runs)\n",
        "        \n",
        "        return {\n",
        "            \"total_cost_usd\": total_cost,\n",
        "            \"total_latency_seconds\": total_latency,\n",
        "            \"average_latency_per_chain\": total_latency / num_chains if num_chains > 0 else 0,\n",
        "            \"llm_calls\": llm_calls,\n",
        "            \"cost_per_chain\": total_cost / num_chains if num_chains > 0 else 0,\n",
        "            \"num_chain_executions\": num_chains\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Retriever Performance Comparison\n",
        "\n",
        "## LangSmith Performance Metrics\n",
        "\n",
        "| Retriever | Date/Time | Runs | error Rate | p50 latency | p99 Latency | streaming | total Tokens | total Cost |\n",
        "|-----------|-----------|------|--------------|-------------|---------------|------------|--------|------|\n",
        "| **ensemble** | 7/29/2025, 9:05:19 AM | 11 | 9% | 10.94s | 22.62s | 0% | 177,088 | $0.02 |\n",
        "| **contextual_compression** | 7/29/2025, 9:01:50 AM | 10 | 0% | 4.39s | 9.75s | 0% | 28,620 | $0.01 |\n",
        "| **parent_document** | 7/29/2025, 9:00:57 AM | 10 | 0% | 5.13s | 10.21s | 0% | 43,063 | $0.01 |\n",
        "| **multi_query** | 7/29/2025, 8:59:56 AM | 10 | 0% | 9.67s | 15.33s | 0% | 129,223 | $0.02 |\n",
        "| **bm25** | 7/29/2025, 8:58:00 AM | 10 | 0% | 3.99s | 8.81s | 0% | 46,168 | $0.01 |\n",
        "| **naive** | 7/29/2025, 8:57:10 AM | 10 | 0% | 7.47s | 11.29s | 0% | 80,432 | $0.01 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "313cffe5dbd44db494d4e0b677dda9f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "result = run_ragas_evaluation(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9000, 'context_precision': 0.8372}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Running RAGAS Evaluation for All Retrievers\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Run RAGAS evaluation for all retrievers\n",
        "print(\"🧪 Running RAGAS Evaluation for All Retrievers\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Store RAGAS results\n",
        "ragas_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, ResponseRelevancy\n",
        "from ragas import evaluate, RunConfig\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "# Configure RAGAS\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "\n",
        "for retriever_name, results in evaluation_results.items():\n",
        "    print(f\"\\n📊 Evaluating {retriever_name.upper()} with RAGAS...\")\n",
        "    \n",
        "    try:\n",
        "        # Get the dataset for this retriever\n",
        "        dataset = results.get(\"dataset\")\n",
        "        \n",
        "        if dataset is None:\n",
        "            print(f\"   ❌ No dataset found for {retriever_name}\")\n",
        "            continue\n",
        "            \n",
        "        # Run RAGAS evaluation\n",
        "        ragas_result = run_ragas_evaluation(dataset)\n",
        "        \n",
        "        # Store results\n",
        "        ragas_results[retriever_name] = ragas_result\n",
        "        \n",
        "        print(f\"   ✅ RAGAS evaluation completed\")\n",
        "        print(f\"   📈 Context Recall: {ragas_result.get('llm_context_recall', 0):.3f}\")\n",
        "        print(f\"   📈 Entity Recall: {ragas_result.get('context_precision', 0):.3f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ RAGAS evaluation failed for {retriever_name}: {e}\")\n",
        "\n",
        "\n",
        "print(f\"\\n✅ RAGAS evaluation completed for all retrievers!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'naive': {'context_recall': 0.9300, 'context_precision': 0.8904},\n",
              " 'bm25': {'context_recall': 0.9071, 'context_precision': 0.8417},\n",
              " 'multi_query': {'context_recall': 0.9750, 'context_precision': 0.8374},\n",
              " 'parent_document': {'context_recall': 0.8136, 'context_precision': 0.9333},\n",
              " 'contextual_compression': {'context_recall': 0.7871, 'context_precision': 0.8917},\n",
              " 'ensemble': {'context_recall': 1.0000, 'context_precision': 0.8709}}"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ragas_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
